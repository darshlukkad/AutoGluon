{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOwEZy3ddKozaCN5PaweDD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshlukkad/AutoGluon/blob/main/california_housing_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9g9aqUrne6q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ebc45d-0c29-41df-89c6-0a807fe84843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Competition: california-house-prices\n",
            "DATA_DIR: /content/data\n",
            "AUTOGLUON_SAVE_PATH: /content/data/AutoGluonModels\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Downloading california-house-prices.zip to /content/data\n",
            "  0% 0.00/29.5M [00:00<?, ?B/s]\n",
            "100% 29.5M/29.5M [00:00<00:00, 1.77GB/s]\n",
            "total 86M\n",
            "-rw-r--r-- 1 root root 248K Mar 19  2021 sample_submission.csv\n",
            "-rw-r--r-- 1 root root  35M Mar 19  2021 test.csv\n",
            "-rw-r--r-- 1 root root  51M Mar 19  2021 train.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ---- CONFIG ----\n",
        "KAGGLE_COMPETITION = \"california-house-prices\"\n",
        "DATA_DIR = \"/content/data\"\n",
        "DATASET = os.path.join(DATA_DIR, KAGGLE_COMPETITION)\n",
        "AUTOGLUON_SAVE_PATH = os.path.join(DATA_DIR, \"AutoGluonModels\")\n",
        "\n",
        "print(\"Competition:\", KAGGLE_COMPETITION)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"AUTOGLUON_SAVE_PATH:\", AUTOGLUON_SAVE_PATH)\n",
        "\n",
        "# ---- Install slim AutoGluon + Kaggle CLI ----\n",
        "!pip install -q kaggle autogluon.tabular scikit-learn\n",
        "\n",
        "# ---- Kaggle auth (assumes kaggle.json exists in Google Drive) ----\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# ---- Download + extract competition files ----\n",
        "!mkdir -p \"{DATA_DIR}\" \"{AUTOGLUON_SAVE_PATH}\"\n",
        "!kaggle competitions download -c \"{KAGGLE_COMPETITION}\" -p \"{DATA_DIR}\" --force\n",
        "!unzip -o -q \"{DATA_DIR}/{KAGGLE_COMPETITION}.zip\" -d \"{DATA_DIR}/{KAGGLE_COMPETITION}\"\n",
        "!rm -f \"{DATA_DIR}/{KAGGLE_COMPETITION}.zip\"\n",
        "!ls -lh \"{DATA_DIR}/{KAGGLE_COMPETITION}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === FAST MODE (Academic demo): California Housing (Optimized) ===\n",
        "import os, time, numpy as np, pandas as pd, random\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---- Config (assumes DATASET & AUTOGLUON_SAVE_PATH already defined) ----\n",
        "SEED = 42\n",
        "N_TRAIN = 20_000        # cap rows for speed\n",
        "TL = 300                # seconds; demo budget\n",
        "THREADS = max(1, (os.cpu_count() or 1))  # use available CPU cores\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "COMP_DIR   = DATASET\n",
        "TRAIN_PATH = os.path.join(COMP_DIR, \"train.csv\")\n",
        "TEST_PATH  = os.path.join(COMP_DIR, \"test.csv\")\n",
        "SUB_PATH   = os.path.join(COMP_DIR, \"sample_submission.csv\")\n",
        "TARGET     = \"Sold Price\"\n",
        "\n",
        "# ---- Load ----\n",
        "train_full = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
        "test_df    = pd.read_csv(TEST_PATH,  low_memory=False)\n",
        "sub_df     = pd.read_csv(SUB_PATH,   low_memory=False)\n",
        "\n",
        "# ---- Deterministic sample for speed ----\n",
        "if len(train_full) > N_TRAIN:\n",
        "    train_df = train_full.sample(n=N_TRAIN, random_state=SEED).reset_index(drop=True)\n",
        "else:\n",
        "    train_df = train_full.reset_index(drop=True)\n",
        "del train_full\n",
        "\n",
        "# ---- Quick preprocess ----\n",
        "# Drop obvious IDs if present\n",
        "id_cols = [c for c in (\"Id\", \"id\") if c in train_df.columns]\n",
        "if id_cols: train_df.drop(columns=id_cols, inplace=True)\n",
        "id_cols_test = [c for c in (\"Id\", \"id\") if c in test_df.columns]\n",
        "if id_cols_test: test_df.drop(columns=id_cols_test, inplace=True)\n",
        "\n",
        "# Log1p-stabilize numerics (except target), then downcast to float32 for speed\n",
        "num_cols_train = train_df.select_dtypes(include=\"number\").columns.tolist()\n",
        "if TARGET in num_cols_train:\n",
        "    num_cols_train.remove(TARGET)\n",
        "\n",
        "train_df[num_cols_train] = np.log1p(train_df[num_cols_train].clip(lower=0))\n",
        "train_df[TARGET]         = np.log1p(train_df[TARGET].clip(lower=0))\n",
        "train_df[num_cols_train] = train_df[num_cols_train].astype(\"float32\")\n",
        "\n",
        "# Match test numeric handling (columns may differ in rare cases)\n",
        "num_cols_test = test_df.select_dtypes(include=\"number\").columns.tolist()\n",
        "test_df[num_cols_test] = np.log1p(test_df[num_cols_test].clip(lower=0))\n",
        "test_df[num_cols_test] = test_df[num_cols_test].astype(\"float32\")\n",
        "\n",
        "# ---- Holdout scheme: 10% final holdout; from remaining 90%, 10% for dev ----\n",
        "train_full_split, holdout = train_test_split(train_df, test_size=0.10, random_state=SEED)\n",
        "train_split, dev_split    = train_test_split(train_full_split, test_size=0.10, random_state=SEED)\n",
        "print(f\"Train: {train_split.shape}, Dev: {dev_split.shape}, Holdout: {holdout.shape}\")\n",
        "\n",
        "# ---- Fast single-model config (LightGBM only) ----\n",
        "# Use more threads for speed, keep seeds for reproducibility.\n",
        "hyperparameters = {\n",
        "    \"GBM\": [{\n",
        "        \"num_boost_round\": 200,\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"num_leaves\": 31,\n",
        "        \"early_stopping_rounds\": 50,   # speeds up if no progress\n",
        "        \"random_state\": SEED,\n",
        "        \"bagging_seed\": SEED,\n",
        "        \"feature_fraction_seed\": SEED,\n",
        "        \"data_random_seed\": SEED,\n",
        "        \"num_threads\": THREADS,        # speed up on multicore\n",
        "    }],\n",
        "    \"CAT\": [], \"XGB\": [], \"RF\": [], \"XT\": [], \"NN_TORCH\": []\n",
        "}\n",
        "\n",
        "ag_args_fit = {\n",
        "    \"random_seed\": SEED,\n",
        "    \"num_cpus\": THREADS,              # propagate thread count\n",
        "}\n",
        "\n",
        "# ---- Output path ----\n",
        "BASE_PATH = AUTOGLUON_SAVE_PATH\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "RUN_NAME = f\"california_house_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "MODEL_PATH = os.path.join(BASE_PATH, RUN_NAME)\n",
        "\n",
        "predictor = TabularPredictor(\n",
        "    label=TARGET,\n",
        "    eval_metric=\"rmse\",\n",
        "    path=MODEL_PATH,\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# ---- Fit ----\n",
        "trained = False\n",
        "try:\n",
        "    predictor.fit(\n",
        "        train_data=train_split,\n",
        "        tuning_data=dev_split,\n",
        "        hyperparameters=hyperparameters,\n",
        "        presets=\"good_quality\",       # slimmer, faster than medium_quality\n",
        "        time_limit=TL,\n",
        "        num_bag_folds=0,\n",
        "        num_stack_levels=0,\n",
        "        keep_only_best=True,\n",
        "        ag_args_fit=ag_args_fit,\n",
        "    )\n",
        "    trained = True\n",
        "except AssertionError as e:\n",
        "    print(\"✅ Fit exited early (likely tight time_limit):\", e)\n",
        "\n",
        "# ---- Evaluate on never-seen holdout + export submission ----\n",
        "if trained and getattr(predictor, \"_trainer\", None):\n",
        "    holdout_metrics = predictor.evaluate(holdout)\n",
        "    print(\"Holdout metrics (log-scale RMSE):\", holdout_metrics)\n",
        "\n",
        "    # Dollar-scale RMSE (original target space)\n",
        "    try:\n",
        "        y_true = np.expm1(holdout[TARGET].to_numpy())\n",
        "        y_pred = np.expm1(predictor.predict(holdout).to_numpy())\n",
        "        rmse_dollars = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "        print(f\"Holdout RMSE (original $): {rmse_dollars:,.2f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Skipping dollar-scale RMSE:\", e)\n",
        "\n",
        "    # Predict test and save\n",
        "    pred_log = predictor.predict(test_df)\n",
        "    pred     = np.expm1(pred_log)\n",
        "    sub      = sub_df.copy()\n",
        "    sub[TARGET] = pred\n",
        "    out_path = os.path.join(MODEL_PATH, \"submission.csv\")\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    print(\"✅ Saved submission:\", out_path)\n",
        "else:\n",
        "    print(\"⏸️ Training didn’t complete — consider increasing TL (≥300 s).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MXVKkmahHq2",
        "outputId": "05f4c5a2-554c-48c8-cc5f-7929e77859ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          8\n",
            "Memory Avail:       49.21 GB / 50.99 GB (96.5%)\n",
            "Disk Space Avail:   195.85 GB / 235.68 GB (83.1%)\n",
            "===================================================\n",
            "Presets specified: ['good_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (16200, 40), Dev: (1800, 40), Holdout: (2000, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 300s\n",
            "AutoGluon will save models to \"/content/data/AutoGluonModels/california_house_20251023_192505\"\n",
            "Train Data Rows:    16200\n",
            "Train Data Columns: 39\n",
            "Tuning Data Rows:    1800\n",
            "Tuning Data Columns: 39\n",
            "Label Column:       Sold Price\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (17.909855136853043, 11.51792295668052, 13.73669, 0.79778)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    50431.20 MB\n",
            "\tTrain Data (Original)  Memory Usage: 35.83 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n",
            "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\tFitting TextNgramFeatureGenerator...\n",
            "\t\t\tFitting CountVectorizer for text features: ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', 'Bedrooms', 'Elementary School', 'Middle School', 'High School', 'Flooring', 'Heating features', 'Cooling features', 'Appliances included', 'Laundry features', 'Parking features']\n",
            "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])                      : 18 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('object', [])                     :  4 | ['Type', 'Region', 'City', 'State']\n",
            "\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
            "\t\t('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t('float', [])                       :   18 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', ['binned', 'text_special']) :  181 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t('int', ['text_ngram'])             : 8877 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t89.4s = Fit runtime\n",
            "\t39 features in original data used to generate 9105 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 311.04 MB (0.6% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 92.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{'num_boost_round': 200, 'learning_rate': 0.1, 'num_leaves': 31, 'early_stopping_rounds': 50, 'random_state': 42, 'bagging_seed': 42, 'feature_fraction_seed': 42, 'data_random_seed': 42, 'num_threads': 8}],\n",
            "\t'CAT': [],\n",
            "\t'XGB': [],\n",
            "\t'RF': [],\n",
            "\t'XT': [],\n",
            "\t'NN_TORCH': [],\n",
            "}\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 207.55s of the 207.55s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=3.0/48.7 GB\n",
            "\t-0.2275\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.24s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 207.55s of the 193.19s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 1.0}\n",
            "\t-0.2275\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 107.23s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 16842.3 rows/s (1800 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_FULL ...\n",
            "\tFitting with cpus=8, gpus=0, mem=3.2/48.2 GB\n",
            "\tWarning: Exception caused LightGBM_FULL to fail during training... Skipping this model.\n",
            "\t\tFor early stopping, at least one dataset and eval metric is required for evaluation\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2171, in _train_and_save\n",
            "    model = self._train_single(**model_fit_kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 300, in _fit\n",
            "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
            "    return lgb.train(**train_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\", line 332, in train\n",
            "    cb(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/callback.py\", line 404, in __call__\n",
            "    self._init(env)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/callback.py\", line 328, in _init\n",
            "    raise ValueError(\"For early stopping, at least one dataset and eval metric is required for evaluation\")\n",
            "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
            "WARNING: Refit training failure detected for 'LightGBM'... Falling back to using first fold to avoid downstream exception.\n",
            "\tThis is likely due to an out-of-memory error or other memory related issue. \n",
            "\tPlease create a GitHub issue if this was triggered from a non-memory related problem.\n",
            "Fitting model: LightGBM_FULL | Skipping fit via cloning parent ...\n",
            "\t-0.2275\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.24s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Updated best model to \"LightGBM_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 9.65s ... Best model: \"LightGBM_FULL\"\n",
            "Deleting model LightGBM. All files under /content/data/AutoGluonModels/california_house_20251023_192505/models/LightGBM will be removed.\n",
            "Deleting model WeightedEnsemble_L2. All files under /content/data/AutoGluonModels/california_house_20251023_192505/models/WeightedEnsemble_L2 will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/data/AutoGluonModels/california_house_20251023_192505\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout metrics (log-scale RMSE): {'root_mean_squared_error': np.float64(-0.1884125617346258), 'mean_squared_error': -0.03549929341940418, 'mean_absolute_error': -0.09152310644919366, 'r2': 0.9436067064340019, 'pearsonr': 0.9715009371785128, 'median_absolute_error': np.float64(-0.04761857970012873)}\n",
            "Holdout RMSE (original $): 795,050.60\n",
            "✅ Saved submission: /content/data/AutoGluonModels/california_house_20251023_192505/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "candidates = sorted(\n",
        "    glob.glob(os.path.join(AUTOGLUON_SAVE_PATH, \"*/submission.csv\")),\n",
        "    key=os.path.getmtime,\n",
        "    reverse=True\n",
        ")\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\"No submission.csv found under AutoGluonModels/*/\")\n",
        "SUBMISSION_FILE = candidates[0]\n",
        "print(\"Using submission file:\", SUBMISSION_FILE)\n",
        "\n",
        "!kaggle competitions submit -c \"california-house-prices\" -f \"$SUBMISSION_FILE\" -m \"submit\"\n",
        "!kaggle competitions submissions -c \"california-house-prices\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtZXRZTriB8d",
        "outputId": "fa36de2a-2bbf-4153-f87b-a86677f9b2f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using submission file: /content/data/AutoGluonModels/california_house_20251023_192505/submission.csv\n",
            "100% 483k/483k [00:01<00:00, 295kB/s]\n",
            "Successfully submitted to California House PricesfileName        date                        description  status                    publicScore  privateScore  \n",
            "--------------  --------------------------  -----------  ------------------------  -----------  ------------  \n",
            "submission.csv  2025-10-23 19:37:03.333000  submit       SubmissionStatus.PENDING                             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYpRAW-KkxYv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}